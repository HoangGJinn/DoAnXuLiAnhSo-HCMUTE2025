import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import tensorflow as tf
import numpy as np
import mediapipe as mp
import pickle
from collections import deque
import av
import cv2
import threading
import pyttsx3
from pathlib import Path

# Load model vÃ  encoder
SL_MODEL_DIR = Path(__file__).parent / 'Source' / 'SignLanguage'
SVM_MODEL_PATH = SL_MODEL_DIR / 'label_encoder.pkl'
MODEL_PATH = SL_MODEL_DIR / 'ASL_model.h5'

model = tf.keras.models.load_model(MODEL_PATH)
with open(SVM_MODEL_PATH, 'rb') as f:
    le = pickle.load(f)

# MediaPipe setup
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# HÃ m Ä‘á»c tá»« sá»­ dá»¥ng pyttsx3
def speak(text):
    if text:
        engine = pyttsx3.init()
        engine.setProperty('rate', 150)  # Tá»‘c Ä‘á»™ nÃ³i
        engine.setProperty('volume', 1.0)  # Ã‚m lÆ°á»£ng (0.0 Ä‘áº¿n 1.0)

        # Thiáº¿t láº­p ngÃ´n ngá»¯ tiáº¿ng Viá»‡t náº¿u cÃ³ cÃ i Ä‘áº·t giá»ng phÃ¹ há»£p
        voices = engine.getProperty('voices')
        for voice in voices:
            if "vi" in voice.id.lower() or "vietnam" in voice.name.lower():
                engine.setProperty('voice', voice.id)
                break

        engine.say(text)
        engine.runAndWait()

# Video Processor
class SignLanguageProcessor(VideoProcessorBase):
    def __init__(self):
        self.hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)
        self.prediction_history = deque(maxlen=10)
        self.current_prediction = ""
        self.confidence = 0.0
        self.predicted_word = ""
        self.check = True

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        result = self.hands.process(image_rgb)

        self.current_prediction = ""
        self.confidence = 0.0

        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                keypoints = []
                for lm in hand_landmarks.landmark:
                    keypoints.extend([lm.x, lm.y, lm.z])

                keypoints_np = np.array(keypoints).reshape(1, -1)
                prediction = model.predict(keypoints_np, verbose=0)
                self.confidence = np.max(prediction)
                predicted_letter = le.inverse_transform([np.argmax(prediction)])[0]
                self.current_prediction = predicted_letter

                if predicted_letter == "next":
                    self.check = True
                    self.prediction_history.clear()
                elif self.confidence > 0.7:
                    self.prediction_history.append(predicted_letter)

                mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # Logic xá»­ lÃ½ tá»«
        if (len(set(self.prediction_history)) == 1
                and len(self.prediction_history) == self.prediction_history.maxlen
                and self.current_prediction not in ["next"]):
            if self.current_prediction == "del":
                self.predicted_word = self.predicted_word[:-1]
                self.prediction_history.clear()
            elif self.check:
                if self.current_prediction == "space":
                    self.predicted_word += " "
                else:
                    self.predicted_word += self.current_prediction
                self.check = False
                self.prediction_history.clear()

        # Ghi thÃ´ng tin lÃªn áº£nh
        text = f"{self.current_prediction} ({self.confidence:.2f})"
        cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        cv2.putText(img, f"Word: {self.predicted_word}", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        cv2.putText(img, f"Check: {self.check}", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        return av.VideoFrame.from_ndarray(img, format="bgr24")

# --- Streamlit UI ---
# --- Khá»Ÿi táº¡o session_state ---
if "show_guide" not in st.session_state:
    st.session_state["show_guide"] = False

# --- Hiá»ƒn thá»‹ pháº§n hÆ°á»›ng dáº«n ---
if st.session_state["show_guide"]:
    with st.expander("ğŸ“˜ HÆ°á»›ng dáº«n sá»­ dá»¥ng", expanded=True):
        col1, col2 = st.columns([2, 1.5])

        with col1:
            st.markdown("""
            **CÃ¡ch sá»­ dá»¥ng á»©ng dá»¥ng:**
            - LÆ°u Ã½: sá»­ dá»¥ng tay pháº£i
            - Nháº¥n nÃºt Start Ä‘á»ƒ báº¯t Ä‘áº§u nháº­n diá»‡n ngÃ´n ngá»¯ kÃ½ hiá»‡u.
            - ÄÆ°a tay vÃ o camera theo kÃ½ hiá»‡u ASL.
            - Khi kÃ½ hiá»‡u á»•n Ä‘á»‹nh, tá»« sáº½ Ä‘Æ°á»£c thÃªm vÃ o káº¿t quáº£.
            - CÃ¡c kÃ½ hiá»‡u Ä‘áº·c biá»‡t:
                - `space`: táº¡o khoáº£ng tráº¯ng.
                - `del`: xÃ³a kÃ½ tá»± cuá»‘i.
                - `next`: cho phÃ©p thÃªm kÃ½ tá»± tiáº¿p theo (â†’ check = True).
            - Chá»‰ khi `check = True`, tá»« má»›i Ä‘Æ°á»£c thÃªm vÃ o.
            - Nháº¥n nÃºt ğŸ”Š Ä‘á»ƒ nghe tá»« Ä‘Æ°á»£c nháº­n diá»‡n.
            - Nháº¥n ğŸ” Ä‘á»ƒ reset láº¡i tá»« hiá»‡n táº¡i.
            """)

        with col2:
            st.image("images/ImageProcessingAdvanced/Sample_ASL.jpg", caption="Minh há»a kÃ½ hiá»‡u tay", use_container_width=True)
        if st.button("ğŸ”½ áº¨n hÆ°á»›ng dáº«n"):
            st.session_state["show_guide"] = False
            st.rerun()
    col1, col2, col3 = st.columns([3,3,2])
    with col1:
        st.image("images/ImageProcessingAdvanced/del_test.jpg", caption="Minh há»a tÃ­nh nÄƒng delete chá»¯")
    with col2:
        st.image("images/ImageProcessingAdvanced/space_test.jpg", caption= "Minh há»a space - khoáº£ng cÃ¡ch")
    with col3:
        st.image("images/ImageProcessingAdvanced/next_test2.png", caption= "Minh há»a check")
# --- NÃºt hiá»ƒn thá»‹ náº¿u Ä‘ang áº©n ---
else:
    if st.button("â“ Hiá»ƒn thá»‹ hÆ°á»›ng dáº«n"):
        st.session_state["show_guide"] = True
        st.rerun()
# --- ThÃªm bá»‘ cá»¥c má»›i ---
st.title("ğŸ§  Nháº­n diá»‡n ngÃ´n ngá»¯ kÃ½ hiá»‡u (ASL)")

col1, col2 = st.columns([2, 1])

with col1:
    webrtc_ctx = webrtc_streamer(
        key="asl-realtime",
        video_processor_factory=SignLanguageProcessor,
        rtc_configuration=RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}),
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )

with col2:
    st.markdown("## ğŸ“Œ Tá»« Ä‘Ã£ nháº­n diá»‡n:")
    if webrtc_ctx.video_processor:
        word = webrtc_ctx.video_processor.predicted_word
        st.markdown(f"<h3 style='color:blue; font-size: 20px'>{word}</h3>", unsafe_allow_html=True)

        if st.button("ğŸ”Š Äá»c tá»«"):
            threading.Thread(target=speak, args=(word,)).start()

        if st.button("ğŸ” Reset tá»«"):
            webrtc_ctx.video_processor.predicted_word = ""
            webrtc_ctx.video_processor.prediction_history.clear()
            webrtc_ctx.video_processor.check = True
